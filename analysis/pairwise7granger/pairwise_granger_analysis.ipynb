{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise VAR and Granger Causality Network Analysis - v12.3\n",
    "\n",
    "**Purpose**: Select top 6-8 variables for VECM analysis based on network centrality from pairwise Granger causality tests.\n",
    "\n",
    "**Dataset**: `complete_normalized_dataset_v12.3.xlsx` (all variables already z-scored)\n",
    "\n",
    "**Analysis Steps**:\n",
    "1. Load 19 normalized variables\n",
    "2. Run pairwise Granger causality tests (lags 1-4)\n",
    "3. Build directed causal network\n",
    "4. Calculate network centrality measures\n",
    "5. Select top variables for VECM modeling\n",
    "6. Visualize network structure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"* 100)\n",
    "print(\"PAIRWISE VAR AND GRANGER CAUSALITY - v12.3 DATASET\")\n",
    "print(\"All variables already normalized (z-scored)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the normalized dataset with all 19 variables (7 military ranks, 5 civilian/political, 3 economic, 4 political party indicators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/6] Loading v12.3 normalized dataset...\")\n",
    "\n",
    "# Load data from parent directory\n",
    "df = pd.read_excel('../complete_normalized_dataset_v12.3.xlsx')\n",
    "\n",
    "# All 19 variables from v12.3\n",
    "analysis_vars = [\n",
    "    'Junior_Enlisted_Z',\n",
    "    'Middle_Enlisted_Z',\n",
    "    'Senior_Enlisted_Z',\n",
    "    'Company_Grade_Officers_Z',\n",
    "    'Field_Grade_Officers_Z',\n",
    "    'GOFOs_Z',\n",
    "    'Warrant_Officers_Z',\n",
    "    'GDP_Growth_Z',\n",
    "    'Major_Conflict',\n",
    "    'Policy_Count_Log',\n",
    "    'Total_Civilians_Z',\n",
    "    'Total_PAS_Z',\n",
    "    'FOIA_Simple_Days_Z',\n",
    "    'Democrat Party HOR',\n",
    "    'Republican Party HOR',\n",
    "    'Democrat Party Senate',\n",
    "    'Republican Party Senate',\n",
    "    'POTUS Democrat Party',\n",
    "    'POTUS Republican Party'\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "available_vars = [v for v in analysis_vars if v in df.columns]\n",
    "print(f\"  Total variables for analysis: {len(available_vars)}\")\n",
    "\n",
    "data = df[available_vars].copy()\n",
    "data = data.dropna()\n",
    "\n",
    "print(f\"  Observations after dropna: {len(data)}\")\n",
    "print(f\"\\n  Available variables:\")\n",
    "for i, var in enumerate(available_vars, 1):\n",
    "    print(f\"    {i:2d}. {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pairwise Granger Causality Tests\n",
    "\n",
    "Test all variable pairs to identify causal relationships using Granger causality (lags 1-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/6] Running pairwise Granger causality tests...\")\n",
    "print(\"  Testing all variable pairs with lags 1-4...\")\n",
    "\n",
    "granger_results = []\n",
    "test_count = 0\n",
    "total_tests = len(available_vars) * (len(available_vars) - 1)\n",
    "\n",
    "for var1 in available_vars:\n",
    "    for var2 in available_vars:\n",
    "        if var1 == var2:\n",
    "            continue\n",
    "\n",
    "        test_count += 1\n",
    "        if test_count % 50 == 0:\n",
    "            print(f\"    Progress: {test_count}/{total_tests} tests completed...\")\n",
    "\n",
    "        try:\n",
    "            test_data = data[[var2, var1]].dropna()\n",
    "\n",
    "            if len(test_data) < 15:\n",
    "                continue\n",
    "\n",
    "            gc_result = grangercausalitytests(test_data, maxlag=4, verbose=False)\n",
    "\n",
    "            for lag in [1, 2, 3, 4]:\n",
    "                if lag in gc_result:\n",
    "                    f_stat = gc_result[lag][0]['ssr_ftest'][0]\n",
    "                    p_val = gc_result[lag][0]['ssr_ftest'][1]\n",
    "\n",
    "                    granger_results.append({\n",
    "                        'Cause': var1,\n",
    "                        'Effect': var2,\n",
    "                        'Lag': lag,\n",
    "                        'F_stat': f_stat,\n",
    "                        'p_value': p_val,\n",
    "                        'Significant_10pct': p_val < 0.10,\n",
    "                        'Significant_5pct': p_val < 0.05,\n",
    "                        'Significant_1pct': p_val < 0.01\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "granger_df = pd.DataFrame(granger_results)\n",
    "\n",
    "print(f\"\\n  Total tests completed: {len(granger_df)}\")\n",
    "print(f\"  Significant at 10%: {granger_df['Significant_10pct'].sum()}\")\n",
    "print(f\"  Significant at 5%: {granger_df['Significant_5pct'].sum()}\")\n",
    "print(f\"  Significant at 1%: {granger_df['Significant_1pct'].sum()}\")\n",
    "\n",
    "# Save results\n",
    "granger_df.to_excel('./pairwise_granger_all_v12.3.xlsx', index=False)\n",
    "\n",
    "granger_sig = granger_df[granger_df['Significant_5pct']].copy()\n",
    "granger_sig.to_excel('./pairwise_granger_significant_v12.3.xlsx', index=False)\n",
    "\n",
    "print(f\"\\n  Saved {len(granger_df)} total tests to pairwise_granger_all_v12.3.xlsx\")\n",
    "print(f\"  Saved {len(granger_sig)} significant tests to pairwise_granger_significant_v12.3.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Causal Network\n",
    "\n",
    "Create a directed network graph from significant Granger causality relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/6] Building directed causal network...\")\n",
    "\n",
    "edges = []\n",
    "for (cause, effect), group in granger_sig.groupby(['Cause', 'Effect']):\n",
    "    max_f = group['F_stat'].max()\n",
    "    min_p = group['p_value'].min()\n",
    "    sig_lags = group['Lag'].tolist()\n",
    "\n",
    "    edges.append({\n",
    "        'source': cause,\n",
    "        'target': effect,\n",
    "        'weight': max_f,\n",
    "        'p_value': min_p,\n",
    "        'significant_lags': ','.join(map(str, sig_lags))\n",
    "    })\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.to_excel('./pairwise_network_edges_v12.3.xlsx', index=False)\n",
    "\n",
    "# Create NetworkX graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(available_vars)\n",
    "\n",
    "for _, edge in edges_df.iterrows():\n",
    "    G.add_edge(edge['source'], edge['target'],\n",
    "               weight=edge['weight'],\n",
    "               p_value=edge['p_value'])\n",
    "\n",
    "print(f\"  Network nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Network edges: {G.number_of_edges()}\")\n",
    "print(f\"  Network density: {nx.density(G):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Network Centrality Measures\n",
    "\n",
    "Compute multiple centrality measures to identify the most important variables in the causal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/6] Calculating network centrality measures...\")\n",
    "\n",
    "# Calculate centrality measures\n",
    "in_degree = dict(G.in_degree())\n",
    "out_degree = dict(G.out_degree())\n",
    "total_degree = {node: in_degree[node] + out_degree[node] for node in G.nodes()}\n",
    "\n",
    "try:\n",
    "    eigenvector = nx.eigenvector_centrality(G, max_iter=1000, weight='weight')\n",
    "except:\n",
    "    eigenvector = {node: 0 for node in G.nodes()}\n",
    "\n",
    "try:\n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "except:\n",
    "    pagerank = {node: 0 for node in G.nodes()}\n",
    "\n",
    "try:\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "except:\n",
    "    betweenness = {node: 0 for node in G.nodes()}\n",
    "\n",
    "# Create centrality dataframe\n",
    "centrality_df = pd.DataFrame({\n",
    "    'Variable': list(G.nodes()),\n",
    "    'In_Degree': [in_degree[n] for n in G.nodes()],\n",
    "    'Out_Degree': [out_degree[n] for n in G.nodes()],\n",
    "    'Total_Degree': [total_degree[n] for n in G.nodes()],\n",
    "    'Eigenvector_Centrality': [eigenvector[n] for n in G.nodes()],\n",
    "    'PageRank': [pagerank[n] for n in G.nodes()],\n",
    "    'Betweenness': [betweenness[n] for n in G.nodes()]\n",
    "})\n",
    "\n",
    "# Normalize centrality measures\n",
    "for col in ['Eigenvector_Centrality', 'PageRank', 'Betweenness']:\n",
    "    if centrality_df[col].max() > 0:\n",
    "        centrality_df[f'{col}_Normalized'] = (centrality_df[col] / centrality_df[col].max()) * 100\n",
    "\n",
    "# Composite score\n",
    "centrality_df['Composite_Score'] = centrality_df[[\n",
    "    'Total_Degree',\n",
    "    'Eigenvector_Centrality_Normalized',\n",
    "    'PageRank_Normalized',\n",
    "    'Betweenness_Normalized'\n",
    "]].mean(axis=1)\n",
    "\n",
    "centrality_df = centrality_df.sort_values('Composite_Score', ascending=False)\n",
    "centrality_df.to_excel('./pairwise_network_centrality_v12.3.xlsx', index=False)\n",
    "\n",
    "print(\"\\n  All variables ranked by composite centrality score:\")\n",
    "print(\"  \" + \"-\" * 96)\n",
    "print(centrality_df[['Variable', 'Total_Degree', 'Composite_Score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Top Variables for VECM\n",
    "\n",
    "Identify top 6-8 variables based on composite centrality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/6] Selecting top variables for VECM model...\")\n",
    "\n",
    "# Try different cutoffs (6, 7, 8 variables)\n",
    "for top_n in [6, 7, 8]:\n",
    "    top_vars = centrality_df.head(top_n)['Variable'].tolist()\n",
    "\n",
    "    print(f\"\\n  TOP {top_n} VARIABLES OPTION:\")\n",
    "    print(\"  \" + \"=\" * 96)\n",
    "    for i, var in enumerate(top_vars, 1):\n",
    "        score = centrality_df[centrality_df['Variable'] == var]['Composite_Score'].values[0]\n",
    "        degree = centrality_df[centrality_df['Variable'] == var]['Total_Degree'].values[0]\n",
    "        print(f\"  {i:2d}. {var:30s} (Composite Score: {score:6.2f}, Degree: {degree:2.0f})\")\n",
    "\n",
    "# Save top 8 as default\n",
    "top_n = 8\n",
    "top_vars = centrality_df.head(top_n)['Variable'].tolist()\n",
    "\n",
    "selection_df = centrality_df.head(top_n)[['Variable', 'Total_Degree', 'Composite_Score']]\n",
    "selection_df.to_excel('./top_variables_for_vecm_v12.3.xlsx', index=False)\n",
    "\n",
    "print(f\"\\n  DEFAULT SELECTION: Top {top_n} variables saved to top_variables_for_vecm_v12.3.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Network\n",
    "\n",
    "Create network diagram and centrality bar chart highlighting top variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/6] Creating network visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 12), facecolor='white')\n",
    "\n",
    "# Network diagram\n",
    "ax = axes[0]\n",
    "pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "\n",
    "node_colors = ['#e74c3c' if node in top_vars else '#95a5a6' for node in G.nodes()]\n",
    "node_sizes = [1500 if node in top_vars else 400 for node in G.nodes()]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.3,\n",
    "                       arrows=True, arrowsize=10, ax=ax)\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
    "                       node_size=node_sizes, alpha=0.8, ax=ax)\n",
    "\n",
    "labels = {node: node if node in top_vars else '' for node in G.nodes()}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=9, font_weight='bold', ax=ax)\n",
    "\n",
    "ax.set_title(f'Granger Causality Network (v12.3) - Top {top_n} Variables Highlighted',\n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "ax.axis('off')\n",
    "\n",
    "# Centrality bar chart\n",
    "ax = axes[1]\n",
    "top_15 = centrality_df.head(15)\n",
    "y_pos = np.arange(len(top_15))\n",
    "\n",
    "ax.barh(y_pos, top_15['Composite_Score'], color='#3498db', alpha=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(top_15['Variable'], fontsize=10)\n",
    "ax.set_xlabel('Composite Centrality Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 15 Variables by Network Centrality (v12.3)',\n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Highlight top 8\n",
    "for i in range(min(top_n, len(top_15))):\n",
    "    ax.get_children()[i].set_color('#e74c3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./pairwise_network_analysis_v12.3.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"  [OK] Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Display final analysis results and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ANALYSIS COMPLETE - v12.3 DATASET\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nPAIRWISE GRANGER CAUSALITY:\")\n",
    "print(f\"  Total variables tested: {len(available_vars)}\")\n",
    "print(f\"  Total pairwise tests: {len(available_vars) * (len(available_vars) - 1)}\")\n",
    "print(f\"  Significant causal relationships (p<0.05): {len(edges_df)}\")\n",
    "print(f\"  Network density: {nx.density(G):.3f}\")\n",
    "\n",
    "print(f\"\\nRECOMMENDED: TOP {top_n} VARIABLES FOR VECM:\")\n",
    "for i, var in enumerate(top_vars, 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FILES GENERATED:\")\n",
    "print(\"=\" * 100)\n",
    "print(\"  1. pairwise_granger_all_v12.3.xlsx - All Granger test results\")\n",
    "print(\"  2. pairwise_granger_significant_v12.3.xlsx - Significant relationships only\")\n",
    "print(\"  3. pairwise_network_edges_v12.3.xlsx - Network edge list\")\n",
    "print(\"  4. pairwise_network_centrality_v12.3.xlsx - Centrality rankings for all variables\")\n",
    "print(\"  5. top_variables_for_vecm_v12.3.xlsx - Top 8 variables recommended for VECM\")\n",
    "print(\"  6. pairwise_network_analysis_v12.3.png - Network visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 100)\n",
    "print(\"  1. Use these 8 variables for VECM analysis\")\n",
    "print(\"  2. Run Johansen cointegration test (lag sensitivity)\")\n",
    "print(\"  3. Estimate VECM model with optimal lag order\")\n",
    "print(\"  4. Run robustness tests and document equations\")\n",
    "print(\"  5. Compare results with v11.8 VECM\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
