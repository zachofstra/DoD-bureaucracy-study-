{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAR(2) Model - DoD Bureaucratic Growth Analysis v12.3",
        "",
        "**Purpose**: Full Vector Autoregression with 2 lags on 8 selected variables.",
        "",
        "**Dataset**: `complete_normalized_dataset_v12.3.xlsx`",
        "",
        "**Analysis Steps**:",
        "1. Load 8 selected variables",
        "2. Test stationarity (ADF tests)",
        "3. Estimate VAR(2) model",
        "4. Extract coefficients",
        "5. Run Granger causality tests",
        "6. Compute Impulse Response Functions",
        "7. Compute Forecast Error Variance Decomposition",
        "",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "from statsmodels.tsa.api import VAR",
        "from statsmodels.tsa.stattools import adfuller",
        "from scipy import stats",
        "from pathlib import Path",
        "import warnings",
        "warnings.filterwarnings('ignore')",
        "",
        "# Configuration",
        "DATA_FILE = '../complete_normalized_dataset_v12.3.xlsx'",
        "OUTPUT_DIR = '.'",
        "LAG_ORDER = 2",
        "",
        "# 8 variables from Granger causality analysis",
        "SELECTED_VARS = [",
        "    'Warrant_Officers_Z',",
        "    'Policy_Count_Log',",
        "    'Company_Grade_Officers_Z',",
        "    'Total_PAS_Z',",
        "    'FOIA_Simple_Days_Z',",
        "    'Junior_Enlisted_Z',",
        "    'Field_Grade_Officers_Z',",
        "    'Total_Civilians_Z'",
        "]",
        "",
        "print(\"=\" * 100)",
        "print(f\"VAR({LAG_ORDER}) MODEL ANALYSIS - v12.3 DATASET\")",
        "print(\"8 Variables Selected from Pairwise Granger Causality\")",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[1/7] Loading data...\")",
        "",
        "df = pd.read_excel(DATA_FILE)",
        "data = df[SELECTED_VARS].dropna()",
        "",
        "print(f\"  Observations: {len(data)}\")",
        "print(f\"  Variables: {len(SELECTED_VARS)}\")",
        "print(\"\\n  Selected variables:\")",
        "for i, var in enumerate(SELECTED_VARS, 1):",
        "    print(f\"    {i}. {var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Stationarity (ADF Tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[2/7] Testing stationarity (ADF tests)...\")",
        "",
        "stationarity_results = []",
        "for var in SELECTED_VARS:",
        "    adf_result = adfuller(data[var], autolag='AIC')",
        "    stationarity_results.append({",
        "        'Variable': var,",
        "        'ADF_Statistic': adf_result[0],",
        "        'p_value': adf_result[1],",
        "        'Lags_Used': adf_result[2],",
        "        'Stationary': 'Yes' if adf_result[1] < 0.05 else 'No'",
        "    })",
        "",
        "stationarity_df = pd.DataFrame(stationarity_results)",
        "stationarity_df.to_excel(f'{OUTPUT_DIR}/stationarity_tests.xlsx', index=False)",
        "",
        "print(\"\\n  Stationarity Test Results:\")",
        "print(\"  \" + \"-\" * 80)",
        "for _, row in stationarity_df.iterrows():",
        "    status = \"[STATIONARY]\" if row['Stationary'] == 'Yes' else \"[NON-STATIONARY]\"",
        "    print(f\"    {row['Variable']:30s} ADF={row['ADF_Statistic']:8.4f}, p={row['p_value']:.4f} {status}\")",
        "",
        "stationary_count = (stationarity_df['Stationary'] == 'Yes').sum()",
        "print(f\"\\n  Stationary variables: {stationary_count}/{len(SELECTED_VARS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estimate VAR(2) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n[3/7] Estimating VAR({LAG_ORDER}) model...\")",
        "",
        "model = VAR(data)",
        "var_result = model.fit(maxlags=LAG_ORDER, ic=None)",
        "",
        "print(f\"\\n  Model estimated successfully\")",
        "print(f\"  Lag order: {var_result.k_ar}\")",
        "print(f\"  Number of equations: {var_result.neqs}\")",
        "print(f\"  Number of coefficients per equation: {var_result.k_ar * var_result.neqs + 1}\")",
        "print(f\"  Total observations used: {var_result.nobs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Diagnostics and R-squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n[4/7] Generating model summary and diagnostics...\")",
        "",
        "# Calculate R-squared for each equation",
        "rsquared_data = []",
        "residuals = var_result.resid.values if hasattr(var_result.resid, 'values') else var_result.resid",
        "for i, var in enumerate(SELECTED_VARS):",
        "    resid = residuals[:, i]",
        "    y_actual = data[var].values[-len(resid):]",
        "    y_pred = y_actual - resid",
        "    ",
        "    ss_res = np.sum(resid ** 2)",
        "    ss_tot = np.sum((y_actual - np.mean(y_actual)) ** 2)",
        "    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0",
        "    ",
        "    n = len(resid)",
        "    k = LAG_ORDER * len(SELECTED_VARS) + 1",
        "    adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k) if (n - k) > 0 else 0",
        "    ",
        "    rsquared_data.append({",
        "        'Variable': var,",
        "        'R_squared': r_squared,",
        "        'Adj_R_squared': adj_r_squared",
        "    })",
        "",
        "rsquared_df = pd.DataFrame(rsquared_data)",
        "rsquared_df.to_excel(f'{OUTPUT_DIR}/model_fit_rsquared.xlsx', index=False)",
        "",
        "print(\"\\n  R-squared by equation:\")",
        "print(\"  \" + \"-\" * 60)",
        "for _, row in rsquared_df.iterrows():",
        "    print(f\"    {row['Variable']:30s} R2={row['R_squared']:.4f}, Adj-R2={row['Adj_R_squared']:.4f}\")",
        "",
        "avg_rsq = rsquared_df['R_squared'].mean()",
        "print(f\"\\n  Average R-squared: {avg_rsq:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract Coefficient Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n[5/7] Extracting coefficient matrices...\")",
        "",
        "for lag in range(1, LAG_ORDER + 1):",
        "    coef_matrix = pd.DataFrame(index=SELECTED_VARS, columns=SELECTED_VARS)",
        "    ",
        "    for to_var in SELECTED_VARS:",
        "        for from_var in SELECTED_VARS:",
        "            param_name = f'L{lag}.{from_var}'",
        "            if param_name in var_result.params.index:",
        "                coef_matrix.loc[from_var, to_var] = var_result.params.loc[param_name, to_var]",
        "            else:",
        "                coef_matrix.loc[from_var, to_var] = 0.0",
        "    ",
        "    coef_matrix = coef_matrix.astype(float)",
        "    coef_matrix.to_excel(f'{OUTPUT_DIR}/coefficients_lag{lag}.xlsx')",
        "    print(f\"  Saved coefficients for lag {lag}\")",
        "",
        "# Save constant terms",
        "const_df = pd.DataFrame({",
        "    'Equation': SELECTED_VARS,",
        "    'Constant': [var_result.params.loc['const', var] for var in SELECTED_VARS]",
        "})",
        "const_df.to_excel(f'{OUTPUT_DIR}/constants.xlsx', index=False)",
        "print(f\"  Saved constant terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Granger Causality Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n[6/7] Running Granger causality tests on fitted model...\")",
        "",
        "granger_results = []",
        "for caused_var in SELECTED_VARS:",
        "    for causing_var in SELECTED_VARS:",
        "        if caused_var == causing_var:",
        "            continue",
        "        ",
        "        try:",
        "            gc_test = var_result.test_causality(caused_var, causing_var, kind='f')",
        "            granger_results.append({",
        "                'Caused': caused_var,",
        "                'Causing': causing_var,",
        "                'F_statistic': gc_test.test_statistic,",
        "                'p_value': gc_test.pvalue,",
        "                'df_num': gc_test.df,",
        "                'df_denom': gc_test.df_denom,",
        "                'Significant_5pct': gc_test.pvalue < 0.05,",
        "                'Significant_1pct': gc_test.pvalue < 0.01",
        "            })",
        "        except:",
        "            continue",
        "",
        "granger_causality_df = pd.DataFrame(granger_results)",
        "",
        "if len(granger_causality_df) > 0:",
        "    granger_causality_df = granger_causality_df.sort_values('p_value')",
        "    granger_causality_df.to_excel(f'{OUTPUT_DIR}/granger_causality_tests.xlsx', index=False)",
        "    ",
        "    sig_5pct = granger_causality_df['Significant_5pct'].sum()",
        "    sig_1pct = granger_causality_df['Significant_1pct'].sum()",
        "    print(f\"\\n  Granger causality test results:\")",
        "    print(f\"    Total tests: {len(granger_causality_df)}\")",
        "    print(f\"    Significant at 5%: {sig_5pct}\")",
        "    print(f\"    Significant at 1%: {sig_1pct}\")",
        "else:",
        "    print(f\"\\n  No Granger causality tests completed successfully\")",
        "    sig_5pct = 0",
        "    sig_1pct = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Impulse Response Functions and FEVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n[7/7] Computing impulse response functions...\")",
        "",
        "# Compute IRFs (10 periods ahead)",
        "irf = var_result.irf(10)",
        "",
        "# Plot IRFs",
        "try:",
        "    fig = irf.plot(orth=False, figsize=(24, 20))",
        "    plt.suptitle('Impulse Response Functions (Non-Orthogonalized) - All Variables',",
        "                 fontsize=16, fontweight='bold', y=0.995)",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])",
        "    plt.savefig(f'{OUTPUT_DIR}/impulse_response_functions_all.png', dpi=300, bbox_inches='tight')",
        "    plt.close()",
        "    print(\"  IRF plots saved\")",
        "except Exception as e:",
        "    print(f\"  Warning: Could not plot IRFs: {e}\")",
        "",
        "# Save IRF data",
        "irf_data = []",
        "for i, impulse_var in enumerate(SELECTED_VARS):",
        "    for j, response_var in enumerate(SELECTED_VARS):",
        "        irf_values = irf.irfs[:, j, i]",
        "        for period in range(len(irf_values)):",
        "            irf_data.append({",
        "                'Impulse': impulse_var,",
        "                'Response': response_var,",
        "                'Period': period,",
        "                'IRF_Value': irf_values[period]",
        "            })",
        "",
        "irf_df = pd.DataFrame(irf_data)",
        "irf_df.to_excel(f'{OUTPUT_DIR}/impulse_response_data.xlsx', index=False)",
        "print(\"  IRF data saved\")",
        "",
        "# FEVD",
        "print(\"\\n  Computing forecast error variance decomposition...\")",
        "fevd = var_result.fevd(10)",
        "",
        "for i, var in enumerate(SELECTED_VARS):",
        "    fevd_var = pd.DataFrame(",
        "        fevd.decomp[:, i, :],",
        "        columns=SELECTED_VARS",
        "    )",
        "    fevd_var.insert(0, 'Period', range(len(fevd_var)))",
        "    fevd_var.to_excel(f'{OUTPUT_DIR}/fevd_{var}.xlsx', index=False)",
        "",
        "print(\"  FEVD saved for all variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 100)",
        "print(f\"VAR({LAG_ORDER}) ANALYSIS COMPLETE\")",
        "print(\"=\" * 100)",
        "",
        "print(f\"\\nMODEL SPECIFICATION:\")",
        "print(f\"  Lag order: {LAG_ORDER}\")",
        "print(f\"  Number of variables: {len(SELECTED_VARS)}\")",
        "print(f\"  Observations used: {var_result.nobs}\")",
        "print(f\"  Total parameters: {len(SELECTED_VARS) * (len(SELECTED_VARS) * LAG_ORDER + 1)}\")",
        "",
        "print(f\"\\nMODEL FIT:\")",
        "print(f\"  Average R-squared: {avg_rsq:.4f}\")",
        "print(f\"  AIC: {var_result.aic:.2f}\")",
        "print(f\"  BIC: {var_result.bic:.2f}\")",
        "print(f\"  HQIC: {var_result.hqic:.2f}\")",
        "",
        "print(f\"\\nDIAGNOSTICS:\")",
        "print(f\"  Stationary variables: {stationary_count}/{len(SELECTED_VARS)}\")",
        "print(f\"  Significant Granger causalities (5%): {sig_5pct}/{len(granger_causality_df)}\")",
        "",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Network Analysis - Create Adjacency Matrices\n",
        "\n",
        "Generate network representations from the VAR(2) coefficients to visualize variable relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\n[8/8] Creating network adjacency matrices...\")\n",
        "\n",
        "# Load coefficient matrices\n",
        "coef_lag1 = pd.read_excel('coefficients_lag1.xlsx', index_col=0)\n",
        "coef_lag2 = pd.read_excel('coefficients_lag2.xlsx', index_col=0)\n",
        "\n",
        "# Combined network (sum of absolute values)\n",
        "adjacency_abs = np.abs(coef_lag1.values) + np.abs(coef_lag2.values)\n",
        "adjacency_abs_df = pd.DataFrame(adjacency_abs, index=SELECTED_VARS, columns=SELECTED_VARS)\n",
        "adjacency_abs_df.to_excel('network_adjacency_matrix_combined.xlsx')\n",
        "print(\"  Saved: network_adjacency_matrix_combined.xlsx\")\n",
        "\n",
        "# Signed network (preserving direction)\n",
        "adjacency_signed = coef_lag1.values + coef_lag2.values\n",
        "adjacency_signed_df = pd.DataFrame(adjacency_signed, index=SELECTED_VARS, columns=SELECTED_VARS)\n",
        "adjacency_signed_df.to_excel('network_adjacency_matrix_signed.xlsx')\n",
        "print(\"  Saved: network_adjacency_matrix_signed.xlsx\")\n",
        "\n",
        "# Create edge list\n",
        "edges = []\n",
        "threshold = 0.05\n",
        "\n",
        "for i, from_var in enumerate(SELECTED_VARS):\n",
        "    for j, to_var in enumerate(SELECTED_VARS):\n",
        "        if i == j:\n",
        "            continue\n",
        "\n",
        "        coef_t1 = coef_lag1.iloc[i, j]\n",
        "        coef_t2 = coef_lag2.iloc[i, j]\n",
        "        magnitude = abs(coef_t1) + abs(coef_t2)\n",
        "\n",
        "        if magnitude > threshold:\n",
        "            signed_sum = coef_t1 + coef_t2\n",
        "            direction = 'Amplifying' if signed_sum > 0 else 'Dampening'\n",
        "\n",
        "            edges.append({\n",
        "                'From': from_var,\n",
        "                'To': to_var,\n",
        "                'Lag1_Coef': coef_t1,\n",
        "                'Lag2_Coef': coef_t2,\n",
        "                'Combined_Magnitude': magnitude,\n",
        "                'Signed_Sum': signed_sum,\n",
        "                'Direction': direction\n",
        "            })\n",
        "\n",
        "edges_df = pd.DataFrame(edges)\n",
        "edges_df = edges_df.sort_values('Combined_Magnitude', ascending=False)\n",
        "edges_df.to_excel('network_edge_list.xlsx', index=False)\n",
        "\n",
        "print(f\"  Saved: network_edge_list.xlsx ({len(edges_df)} edges)\")\n",
        "print(f\"  Threshold: |coef| > {threshold}\")\n",
        "\n",
        "# Display top relationships\n",
        "print(\"\nTOP 10 STRONGEST RELATIONSHIPS:\")\n",
        "print(\"-\" * 80)\n",
        "for idx, row in edges_df.head(10).iterrows():\n",
        "    print(f\"  {row['From']:30s} -> {row['To']:30s}: {row['Combined_Magnitude']:6.3f} ({row['Direction']})\")\n",
        "\n",
        "edges_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Network Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_possible = len(SELECTED_VARS) * (len(SELECTED_VARS) - 1)\n",
        "density = len(edges_df) / total_possible\n",
        "\n",
        "print(\"\nNETWORK STATISTICS:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Nodes (variables): {len(SELECTED_VARS)}\")\n",
        "print(f\"Edges (relationships > {threshold}): {len(edges_df)}\")\n",
        "print(f\"Total possible directed edges: {total_possible}\")\n",
        "print(f\"Network density: {density:.3f}\")\n",
        "\n",
        "# Degree centrality\n",
        "in_degree = edges_df['To'].value_counts()\n",
        "out_degree = edges_df['From'].value_counts()\n",
        "\n",
        "print(\"\nMost Influenced (In-Degree):\")\n",
        "for var, count in in_degree.head(5).items():\n",
        "    print(f\"  {var:40s}: {count} incoming edges\")\n",
        "\n",
        "print(\"\nMost Influential (Out-Degree):\")\n",
        "for var, count in out_degree.head(5).items():\n",
        "    print(f\"  {var:40s}: {count} outgoing edges\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}